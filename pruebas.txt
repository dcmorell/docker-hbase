docker cp test-hive-hbase/data_1.csv hive-server:/opt/ \
	&& docker cp test-hive-hbase/ddl-hive.sql hive-server:/opt/ \
	&& docker-compose exec hive-server bash


docker-compose exec hbase-master bash

hbase shell

CREATE 'tbl_hbase_1', 'info', 'nav'


---
WITH SERDEPROPERTIES ("hbase.columns.mapping" = ":key,info:n,info:a,info:s,nav:u,nav:s,nav:y,nav:m,nav:d,nav:h")
TBLPROPERTIES ("hbase.table.name" = "tbl_hbase_1", "hbase.mapred.output.outputtable" = "tbl_hbase_1");


<property>
    <name>hive.aux.jars.path</name>
    <value>file:///usr/lib/hive/apache-hive-0.13.1-bin/lib/zookeeper-3.4.5.jar,file:///usr/lib/hive/apache-hive-0.13.1-bin/lib/hive-hbase-handler-0.13.1.jar,file:///usr/lib/hive/apache-hive-0.13.1-bin/lib/guava-11.0.2.jar,file:///usr/lib/hbase/hbase-0.98.2-hadoop2/lib/hbase-client-0.98.2-hadoop2.jar,file:///usr/lib/hbase/hbase-0.98.2-hadoop2/lib/hbase-common-0.98.2-hadoop2.jar,file:///usr/lib/hbase/hbase-0.98.2-hadoop2/lib/hbase-protocol-0.98.2-hadoop2.jar,file:///usr/lib/hbase/hbase-0.98.2-hadoop2/lib/hbase-server-0.98.2-hadoop2.jar,file:///usr/lib/hbase/hbase-0.98.2-hadoop2/lib/hbase-shell-0.98.2-hadoop2.jar,file:///usr/lib/hbase/hbase-0.98.2-hadoop2/lib/hbase-thrift-0.98.2-hadoop2.jar</value>
  </property>


mvn org.apache.maven.plugins:maven-dependency-plugin:2.1:get \
    -DrepoUrl=http://download.java.net/maven/2/ \
    -Dartifact=org.apache.hive:hive-hbase-handler:1.1.1

wget -P /opt https://repo.maven.apache.org/maven2/org/apache/hive/hive-hbase-handler/1.1.0/hive-hbase-handler-1.1.0.jar 

wget https://repo.maven.apache.org/maven2/org/apache/hbase/hbase-hadoop-compat/1.2.0/hbase-hadoop-compat-1.2.0.jar

wget https://repo.maven.apache.org/maven2/org/apache/hbase/hbase-hadoop2-compat/1.2.0/hbase-hadoop2-compat-1.2.0.jar

wget https://repo.maven.apache.org/maven2/org/apache/hbase/hbase-protocol/1.2.12/hbase-protocol-1.2.12.jar

wget https://repo.maven.apache.org/maven2/org/apache/hbase/hbase-server/1.2.0/hbase-server-1.2.0.jar

wget https://repo.maven.apache.org/maven2/org/apache/hbase/hbase-thrift/1.2.0/hbase-thrift-1.2.0.jar

wget https://repo.maven.apache.org/maven2/org/apache/hbase/hbase-shell/1.2.0/hbase-shell-1.2.0.jar

wget https://repo.maven.apache.org/maven2/org/apache/hbase/hbase-prefix-tree/1.2.0/hbase-prefix-tree-1.2.0.jar

wget https://repo.maven.apache.org/maven2/org/apache/hbase/hbase-it/1.2.0/hbase-it-1.2.0.jar

wget https://repo.maven.apache.org/maven2/org/apache/htrace/htrace-core/3.1.0-incubating/htrace-core-3.1.0-incubating.jar

wget https://repo1.maven.org/maven2/org/apache/htrace/htrace-core/3.1.0-incubating/htrace-core-3.1.0-incubating.jar

wget https://repo1.maven.org/maven2/com/codahale/metrics/metrics-core/2.2.0/metrics-core-2.2.0.jar

wget -P lib https://repo.maven.apache.org/maven2/org/apache/hbase/hbase-metrics-api/1.2.0/hbase-metrics-api-1.2.0.jar

wget -P lib https://repo1.maven.org/maven2/com/yammer/metrics/metrics-core/2.2.0/metrics-core-2.2.0.jar

wget https://repo.maven.apache.org/maven2/org/apache/hbase/hbase/1.2.0/hbase-1.2.0.jar


wget https://repo1.maven.org/maven2/com/google/guava/guava/15.0/guava-15.0.jar


find ./ -name "*.jar" | xargs grep Gauge.class



mvn org.apache.maven.plugins:maven-dependency-plugin:2.1:get \
    -DrepoUrl=http://download.java.net/maven/2/ \
    -Dartifact=org.apache.hbase:hbase-client:1.2.0

mvn org.apache.maven.plugins:maven-dependency-plugin:2.1:get \
    -DrepoUrl=http://download.java.net/maven/2/ \
    -Dartifact=org.apache.hbase:hbase-common:1.2.0

mvn org.apache.maven.plugins:maven-dependency-plugin:2.1:get \
    -DrepoUrl=http://download.java.net/maven/2/ \
    -Dartifact=org.apache.hbase.connectors.spark:hbase-spark:1.0.0

??    https://mvnrepository.com/artifact/org.apache.hbase.connectors.spark/hbase-spark 

mvn org.apache.maven.plugins:maven-dependency-plugin:2.1:get \
    -DrepoUrl=https://repository.cloudera.com/artifactory/cloudera-repos/ \
    -Dartifact=org.apache.hbase:hbase-spark:1.2.0-cdh5.14.4

-DrepoUrl=http://maven.aliyun.com/nexus/content/groups/public/ \
-DrepoUrl=https://mvnrepository.com/artifact/ \
-DrepoUrl=https://repository.cloudera.com/artifactory/cloudera-repos/ \
### mvn org.apache.maven.plugins:maven-dependency-plugin:2.1:get \
    -DrepoUrl=http://download.java.net/maven/2/ \
    -Dartifact=org.apache.hbase:hbase-server:1.2.0


sudo netstat -tulpn | grep -w ':9000'
sudo netstat -tulpn | grep -w ':2181'

 http://localhost:50070/
 hdfs://localhost:9000/

spark-shell --jars \
/home/dcastrom/.m2/repository/org/apache/hbase/hbase-common/1.2.0/hbase-common-1.2.0.jar,\
/home/dcastrom/.m2/repository/org/apache/hbase/hbase-client/1.2.0/hbase-client-1.2.0.jar,\
/home/dcastrom/.m2/repository/org/apache/hbase/connectors/spark/hbase-spark/1.0.0/hbase-spark-1.0.0.jar

// /home/dcastrom/.m2/repository/org/apache/hbase/hbase-spark/1.2.0-cdh5.14.4/hbase-spark-1.2.0-cdh5.14.4.jar

sc.listJars


import org.apache.hadoop.hbase.spark.HBaseContext
import org.apache.hadoop.hbase.HBaseConfiguration

val conf = new HBaseConfiguration()
conf.set("hbase.zookeeper.quorum", "locahost")
conf.set("hbase.zookeeper.property.clientPort", "2181")
new HBaseContext(spark.sparkContext, conf)

val hbaseTable = "default:tbl1"

val columnMapping = """id string :key,
  |col1 string cf:col1""".stripMargin

val hbaseSource = "org.apache.hadoop.hbase.spark"

val hbaseData = spark.read.format(hbaseSource).option("hbase.columns.mapping", columnMapping).option("hbase.table", hbaseTable)

val hbaseDf = hbaseData.load()



https://torredecontrol.si.orange.es/gitlab/BIGDATA/GEA_OSP/DESARROLLO_DATOS/v360_hbase_reader







## EN HBase shell
create 'tbl1', 'cf'

val format1 = "org.apache.hadoop.hbase.spark"
val fomat2 = "'org.apache.spark.sql.execution.datasources.hbase'"

val catalog = """{
    "table":{"namespace":"default", "name":"tbl1"},
    "rowkey":"key",
    "columns":{
        "col0":{"cf":"rowkey", "col":"key", "type":"string"},
        "col1":{"cf":"cf", "col":"col1", "type":"string"}
    }
}""".split()


df = sc.parallelize([('a', '1.0'), ('b', '2.0')]).toDF(schema=['col0', 'col1'])

df.write.options(catalog=catalog).format(format1).save()













cp /home/dcastrom/.m2/repository/org/apache/hbase/hbase-common/1.2.0/hbase-common-1.2.0.jar hive/lib && cp /home/dcastrom/.m2/repository/org/apache/hbase/hbase-client/1.2.0/hbase-client-1.2.0.jar hive/lib


hive FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. org/apache/hadoop/hbase/HBaseConfiguration


FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. org/apache/hadoop/hbase/mapreduce/TableInputFormatBase


spark-shell --master yarn



RUN  groupadd -g 1000 hadoop && useradd -u 1000 -g 1000 hadoop
..
RUN chmown hadoop:hadoop /opt/h


export HADOOP_CONF_DIR=/home/dcastrom/work/repos/docker-hbase-hive.git/hadoop-conf && spark-shell --master yarn --deploy-mode client --conf spark.hadoop.yarn.timeline-service.enabled=false --conf spark.yarn.jars=/home/dcastrom/apps/spark-2.4.7-bin-hadoop2.7/jars/spark-yarn_2.11-2.4.7.jar




spark-shell --conf spark.sql.hive.hiveserver2.jdbc.url="jdbc:hive2://hive-server:10000/" 


spark.datasource.hive.warehouse.load.staging.dir="/tmp" spark.hadoop.hive.zookeeper.quorum="ip.************:2181"




spark-shell --conf spark.sql.catalogImplementation=hive hive.metastore.uris=jdbc:mysql://localhost:3306/metastore?createDatabaseIfNotExist=true


hbase-client-1.2.0.jar  hbase-hadoop2-compat-1.2.0.jar  hbase-it-1.2.0.jar           hbase-protocol-1.2.0.jar  hbase-shell-1.2.0.jar   hive-hbase-handler-1.1.0.jar      metrics-core-3.0.2.jar
hbase-common-1.2.0.jar  hbase-hadoop-compat-1.2.0.jar   hbase-prefix-tree-1.2.0.jar  hbase-server-1.2.0.jar    hbase-thrift-1.2.0.jar  htrace-core-3.1.0-incubating.jar

jars en hive/lib
hbase-client-1.2.0.jar  hbase-hadoop2-compat-1.2.0.jar  hbase-it-1.2.0.jar           hbase-protocol-1.2.0.jar  hbase-shell-1.2.0.jar   hive-hbase-handler-1.1.0.jar      metrics-core-3.0.2.jar
hbase-common-1.2.0.jar  hbase-hadoop-compat-1.2.0.jar   hbase-prefix-tree-1.2.0.jar  hbase-server-1.2.0.jar    hbase-thrift-1.2.0.jar  htrace-core-3.1.0-incubating.jar

#####################3

wget -P hive/lib https://repo.maven.apache.org/maven2/org/apache/hbase/hbase-shaded-client/1.2.0/hbase-shaded-client-1.2.0.jar

?guava-15.0.jar,\

wget -P hive/lib https://repo.maven.apache.org/maven2/org/apache/hbase/hbase-shaded-server/1.2.0/hbase-shaded-server-1.2.0.jar


spark-shell --conf spark.sql.hive.hiveserver2.jdbc.url="jdbc:hive2://hive-server:10000/" \
--jars \
hive/lib/hbase-common-1.2.0.jar,\
hive/lib/hbase-shaded-client-1.2.0.jar,\
hive/lib/hbase-protocol-1.2.0.jar,\
hive/lib/hbase-hadoop2-compat-1.2.0.jar,\
hive/lib/metrics-core-2.2.0.jar



// Read Hive Table
spark.sql("USE bd1")
spark.sql("SHOW TABLES").show()
val rawData = spark.sql("SELECT * FROM bd1.data_raw")
rawData.show()

// Config
import org.apache.hadoop.hbase.HBaseConfiguration
import org.apache.hadoop.hbase.client.HBaseAdmin
import org.apache.hadoop.hbase.client.HTable
import org.apache.hadoop.hbase.client.Get
import org.apache.hadoop.hbase.client.Result
import org.apache.hadoop.hbase.util.Bytes


import org.apache.hadoop.hbase.client.Connection;
import org.apache.hadoop.hbase.client.ConnectionFactory;
import org.apache.hadoop.hbase.client.Result;
import org.apache.hadoop.hbase.client.ResultScanner;
import org.apache.hadoop.hbase.client.Scan;
import org.apache.hadoop.hbase.client.Table;

val conf = HBaseConfiguration.create()
conf.set("hbase.zookeeper.quorum", "zoo")
conf.set("hbase.zookeeper.property.clientPort", "2181")

val hbaseOk = HBaseAdmin.checkHBaseAvailable(conf)

val hTbl = new HTable(conf, "space1:data")
val hTbl1 = new HTable(conf, "tbl1")


val g = new Get(Bytes.toBytes("001"))
val result = hTbl1.get(g)


#### HBase
dc exec hbase-master hbase shell

~~~
create_namespace 'space1'

create 'space1:data', 'info', 'nav'
~~~

scan 'space1:data'


~~~
disable 'space1:data'
drop 'space1:data'